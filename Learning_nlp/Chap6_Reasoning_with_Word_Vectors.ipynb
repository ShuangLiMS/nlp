{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vecotors\n",
    "Word vectors: numerical representation of the semantics of words. \n",
    "Instead of training the neural network to predict the meaning of the input words, the neural network is trained to predict words near the target word in the unlabeled sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA or LDiA are good for documents classification, semantic search, and clustering, but the word vectors produced through them are not accurate enough to be used for semantic reasoning or classification and clustering of short phrases or compound words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two ways to train word2ve embeddings: \n",
    "1. skip-gram: predicts the context of words (output words) from a word of interest(the input word)\n",
    "2. Continuous Bag of Words (CBOW) predicts the target word (the output word) from the newby words(input words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-gram Approach: predict surrounding word from the given word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional n-grams language model works in terms of discrete units that have no inherent relationship to one another, a continuous space model works in terms of word vectors where similar words are likely to have similar vectos.\n",
    "\n",
    "Input: One hot encoding, dimension: volcabulary\n",
    "Hidden: Recurrent NN\n",
    "Output: Softmax, dimension: volcabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Bag of Words Approach: predict central word from surrouding words\n",
    "Continuous bag of words changes while sliding over the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-gram works well for small corpoa and rare term, while continuous bags of words work well for frequent word and is faster to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvements on top of word2vec\n",
    "1. Frequency Bi-Grams: frequently co-occurent 2-gram/3-grams are included as independent word in word2vec\n",
    "2. Subsampling frequent tokens: common words like 'a' and 'the that don't carry significant information are sampled less often\n",
    "3. Negative subsampling: If you train your word model with a small corpus, you might want to use a negative sampling rate of 5-20 samples. For larger corpi, you can reduce the sample rate as low as 2-5 samples as suggested by Mikolov and his team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('/Users/sli/Projects/data/GoogleNews-vectors-negative300.bin', binary=True, \n",
    "                                                limit=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('cook', 0.6973531246185303),\n",
       " ('sweet_potatoes', 0.6600280404090881),\n",
       " ('vegetables', 0.6513738632202148),\n",
       " ('onions', 0.6512383818626404),\n",
       " ('baking', 0.6481684446334839)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['cooking', 'potatoes'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('harms', 0.7176095247268677),\n",
       " ('harming', 0.6956215500831604),\n",
       " ('endanger', 0.6636940836906433),\n",
       " ('irreparable_damage', 0.6374310851097107),\n",
       " ('harmed', 0.6304865479469299),\n",
       " ('injure', 0.5920926928520203),\n",
       " ('harmful', 0.5859167575836182),\n",
       " ('damage', 0.5471370220184326),\n",
       " ('detrimental', 0.5405454635620117),\n",
       " ('irreparable_harm', 0.5390825867652893)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['harm'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'computer'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.doesnt_match(\"potatoes milk cake computer\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071), ('monarch', 0.6189674735069275)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['king', 'woman'], negative=['man'], topn=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors['phone'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a new Word vector representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tfidf and words counts, the concept of sentences have mostly been dropped. However, for the creation and learning of word2vec, the concept of sentence is very import. Therefore, the first step in creating the domain-specific word-vec is to convert the document/documents in to list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list =[\n",
    "  ['to', 'provide', 'early', 'intervention/early', 'childhood', 'special', 'education',\n",
    "   'services', 'to', 'eligible', 'children', 'and', 'their', 'families'],\n",
    "  ['essential', 'job', 'functions'],\n",
    "  ['participate', 'as', 'a', 'transdisciplinary', 'team', 'member', 'to', 'complete',\n",
    "   'educational', 'assessments', 'for']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300\n",
    "min_word_count = 3\n",
    "num_workers = 2\n",
    "window_size = 6\n",
    "subsampling = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    token_list,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=window_size,\n",
    "    sample=subsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
